{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce4e24c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-cd9e52abdae4>:139: RuntimeWarning: invalid value encountered in true_divide\n",
      "  value_list /= result_score\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intern': 0.5672438812072306, 'organ': 0.6431492047837458, 'crime': 0.5143865080063751}\n",
      "{'intern': 0.5672438812072306, 'organ': 0.6431492047837458, 'crime': 0.5143865080063751}\n",
      "{'intern': 0.5672438812072306, 'organ': 0.6431492047837458, 'crime': 0.5143865080063751}\n",
      "{'poliomyel': 0.5312340740155382, 'and': 0.39162619398421356, 'post': 0.5312340740155382, 'polio': 0.5312340740155382}\n",
      "{'poliomyel': 0.5312340740155382, 'and': 0.39162619398421356, 'post': 0.5312340740155382, 'polio': 0.5312340740155382}\n",
      "{'poliomyel': 0.5312340740155382, 'and': 0.39162619398421356, 'post': 0.5312340740155382, 'polio': 0.5312340740155382}\n",
      "{'poliomyel': 0.5312340740155382, 'and': 0.39162619398421356, 'post': 0.5312340740155382, 'polio': 0.5312340740155382}\n",
      "{'hubbl': 0.5773502691896257, 'telescop': 0.5773502691896257, 'achiev': 0.5773502691896257}\n",
      "{'hubbl': 0.5773502691896257, 'telescop': 0.5773502691896257, 'achiev': 0.5773502691896257}\n",
      "{'hubbl': 0.5773502691896257, 'telescop': 0.5773502691896257, 'achiev': 0.5773502691896257}\n",
      "{'endang': 0.5773502691896257, 'speci': 0.5773502691896257, 'mammal': 0.5773502691896257}\n",
      "{'endang': 0.5773502691896257, 'speci': 0.5773502691896257, 'mammal': 0.5773502691896257}\n",
      "{'endang': 0.5773502691896257, 'speci': 0.5773502691896257, 'mammal': 0.5773502691896257}\n",
      "{'most': 0.5773502691896257, 'danger': 0.5773502691896257, 'vehicl': 0.5773502691896257}\n",
      "{'most': 0.5773502691896257, 'danger': 0.5773502691896257, 'vehicl': 0.5773502691896257}\n",
      "{'most': 0.5773502691896257, 'danger': 0.5773502691896257, 'vehicl': 0.5773502691896257}\n",
      "{'african': 0.5999882716312075, 'civilian': 0.5999882716312075, 'death': 0.5291768587249377}\n",
      "{'african': 0.5999882716312075, 'civilian': 0.5999882716312075, 'death': 0.5291768587249377}\n",
      "{'african': 0.5999882716312075, 'civilian': 0.5999882716312075, 'death': 0.5291768587249377}\n",
      "{'new': 0.5291768587249377, 'hydroelectr': 0.5999882716312075, 'project': 0.5999882716312075}\n",
      "{'new': 0.5291768587249377, 'hydroelectr': 0.5999882716312075, 'project': 0.5999882716312075}\n",
      "{'new': 0.5291768587249377, 'hydroelectr': 0.5999882716312075, 'project': 0.5999882716312075}\n",
      "{'implant': 0.7071067811865475, 'dentistri': 0.7071067811865475}\n",
      "{'implant': 0.7071067811865475, 'dentistri': 0.7071067811865475}\n",
      "{'rap': 0.6767990587651588, 'and': 0.49893681983310545, 'crime': 0.5412994401155634}\n",
      "{'rap': 0.6767990587651588, 'and': 0.49893681983310545, 'crime': 0.5412994401155634}\n",
      "{'rap': 0.6767990587651588, 'and': 0.49893681983310545, 'crime': 0.5412994401155634}\n",
      "{'radio': 0.4691442570527884, 'wave': 0.4691442570527884, 'and': 0.3458535301215657, 'brain': 0.4691442570527884, 'cancer': 0.4691442570527884}\n",
      "{'radio': 0.4691442570527884, 'wave': 0.4691442570527884, 'and': 0.3458535301215657, 'brain': 0.4691442570527884, 'cancer': 0.4691442570527884}\n",
      "{'radio': 0.4691442570527884, 'wave': 0.4691442570527884, 'and': 0.3458535301215657, 'brain': 0.4691442570527884, 'cancer': 0.4691442570527884}\n",
      "{'radio': 0.4691442570527884, 'wave': 0.4691442570527884, 'and': 0.3458535301215657, 'brain': 0.4691442570527884, 'cancer': 0.4691442570527884}\n",
      "{'radio': 0.4691442570527884, 'wave': 0.4691442570527884, 'and': 0.3458535301215657, 'brain': 0.4691442570527884, 'cancer': 0.4691442570527884}\n",
      "{'industri': 0.7071067811865475, 'espionag': 0.7071067811865475}\n",
      "{'industri': 0.7071067811865475, 'espionag': 0.7071067811865475}\n",
      "{'hydropon': 1.0}\n",
      "{'magnet': 0.5773502691896257, 'levit': 0.5773502691896257, 'maglev': 0.5773502691896257}\n",
      "{'magnet': 0.5773502691896257, 'levit': 0.5773502691896257, 'maglev': 0.5773502691896257}\n",
      "{'magnet': 0.5773502691896257, 'levit': 0.5773502691896257, 'maglev': 0.5773502691896257}\n",
      "{'marin': 0.7071067811865475, 'veget': 0.7071067811865475}\n",
      "{'marin': 0.7071067811865475, 'veget': 0.7071067811865475}\n",
      "{'unexplain': 0.5773502691896257, 'highway': 0.5773502691896257, 'accid': 0.5773502691896257}\n",
      "{'unexplain': 0.5773502691896257, 'highway': 0.5773502691896257, 'accid': 0.5773502691896257}\n",
      "{'unexplain': 0.5773502691896257, 'highway': 0.5773502691896257, 'accid': 0.5773502691896257}\n",
      "{'polygami': 0.5773502691896257, 'polyandri': 0.5773502691896257, 'polygyni': 0.5773502691896257}\n",
      "{'polygami': 0.5773502691896257, 'polyandri': 0.5773502691896257, 'polygyni': 0.5773502691896257}\n",
      "{'polygami': 0.5773502691896257, 'polyandri': 0.5773502691896257, 'polygyni': 0.5773502691896257}\n",
      "{'unsolicit': 0.7071067811865475, 'fax': 0.7071067811865475}\n",
      "{'unsolicit': 0.7071067811865475, 'fax': 0.7071067811865475}\n",
      "{'best': 0.5773502691896257, 'retir': 0.5773502691896257, 'countri': 0.5773502691896257}\n",
      "{'best': 0.5773502691896257, 'retir': 0.5773502691896257, 'countri': 0.5773502691896257}\n",
      "{'best': 0.5773502691896257, 'retir': 0.5773502691896257, 'countri': 0.5773502691896257}\n",
      "{'new': 0.5291768587249377, 'fuel': 0.5999882716312075, 'sourc': 0.5999882716312075}\n",
      "{'new': 0.5291768587249377, 'fuel': 0.5999882716312075, 'sourc': 0.5999882716312075}\n",
      "{'new': 0.5291768587249377, 'fuel': 0.5999882716312075, 'sourc': 0.5999882716312075}\n",
      "{'undersea': 0.5, 'fiber': 0.5, 'optic': 0.5, 'cabl': 0.5}\n",
      "{'undersea': 0.5, 'fiber': 0.5, 'optic': 0.5, 'cabl': 0.5}\n",
      "{'undersea': 0.5, 'fiber': 0.5, 'optic': 0.5, 'cabl': 0.5}\n",
      "{'undersea': 0.5, 'fiber': 0.5, 'optic': 0.5, 'cabl': 0.5}\n",
      "{'women': 0.5773502691896257, 'in': 0.5773502691896257, 'parliament': 0.5773502691896257}\n",
      "{'women': 0.5773502691896257, 'in': 0.5773502691896257, 'parliament': 0.5773502691896257}\n",
      "{'women': 0.5773502691896257, 'in': 0.5773502691896257, 'parliament': 0.5773502691896257}\n",
      "{'intern': 0.5672438812072306, 'art': 0.6431492047837458, 'crime': 0.5143865080063751}\n",
      "{'intern': 0.5672438812072306, 'art': 0.6431492047837458, 'crime': 0.5143865080063751}\n",
      "{'intern': 0.5672438812072306, 'art': 0.6431492047837458, 'crime': 0.5143865080063751}\n",
      "{'literari': 0.5773502691896257, 'journalist': 0.5773502691896257, 'plagiar': 0.5773502691896257}\n",
      "{'literari': 0.5773502691896257, 'journalist': 0.5773502691896257, 'plagiar': 0.5773502691896257}\n",
      "{'literari': 0.5773502691896257, 'journalist': 0.5773502691896257, 'plagiar': 0.5773502691896257}\n",
      "{'argentin': 0.5773502691896257, 'british': 0.5773502691896257, 'relat': 0.5773502691896257}\n",
      "{'argentin': 0.5773502691896257, 'british': 0.5773502691896257, 'relat': 0.5773502691896257}\n",
      "{'argentin': 0.5773502691896257, 'british': 0.5773502691896257, 'relat': 0.5773502691896257}\n",
      "{'cult': 0.7071067811865475, 'lifestyl': 0.7071067811865475}\n",
      "{'cult': 0.7071067811865475, 'lifestyl': 0.7071067811865475}\n",
      "{'ferri': 0.7071067811865475, 'sink': 0.7071067811865475}\n",
      "{'ferri': 0.7071067811865475, 'sink': 0.7071067811865475}\n",
      "{'modern': 0.7071067811865475, 'slaveri': 0.7071067811865475}\n",
      "{'modern': 0.7071067811865475, 'slaveri': 0.7071067811865475}\n",
      "{'pope': 0.7071067811865475, 'beatif': 0.7071067811865475}\n",
      "{'pope': 0.7071067811865475, 'beatif': 0.7071067811865475}\n",
      "{'mexican': 0.5773502691896257, 'air': 0.5773502691896257, 'pollut': 0.5773502691896257}\n",
      "{'mexican': 0.5773502691896257, 'air': 0.5773502691896257, 'pollut': 0.5773502691896257}\n",
      "{'mexican': 0.5773502691896257, 'air': 0.5773502691896257, 'pollut': 0.5773502691896257}\n",
      "{'iran': 0.5773502691896257, 'iraq': 0.5773502691896257, 'cooper': 0.5773502691896257}\n",
      "{'iran': 0.5773502691896257, 'iraq': 0.5773502691896257, 'cooper': 0.5773502691896257}\n",
      "{'iran': 0.5773502691896257, 'iraq': 0.5773502691896257, 'cooper': 0.5773502691896257}\n",
      "{'world': 0.5773502691896257, 'bank': 0.5773502691896257, 'critic': 0.5773502691896257}\n",
      "{'world': 0.5773502691896257, 'bank': 0.5773502691896257, 'critic': 0.5773502691896257}\n",
      "{'world': 0.5773502691896257, 'bank': 0.5773502691896257, 'critic': 0.5773502691896257}\n",
      "{'incom': 0.5773502691896257, 'tax': 0.5773502691896257, 'evas': 0.5773502691896257}\n",
      "{'incom': 0.5773502691896257, 'tax': 0.5773502691896257, 'evas': 0.5773502691896257}\n",
      "{'incom': 0.5773502691896257, 'tax': 0.5773502691896257, 'evas': 0.5773502691896257}\n",
      "{'antibiot': 0.5773502691896257, 'bacteria': 0.5773502691896257, 'diseas': 0.5773502691896257}\n",
      "{'antibiot': 0.5773502691896257, 'bacteria': 0.5773502691896257, 'diseas': 0.5773502691896257}\n",
      "{'antibiot': 0.5773502691896257, 'bacteria': 0.5773502691896257, 'diseas': 0.5773502691896257}\n",
      "{'export': 0.5773502691896257, 'control': 0.5773502691896257, 'cryptographi': 0.5773502691896257}\n",
      "{'export': 0.5773502691896257, 'control': 0.5773502691896257, 'cryptographi': 0.5773502691896257}\n",
      "{'export': 0.5773502691896257, 'control': 0.5773502691896257, 'cryptographi': 0.5773502691896257}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adopt': 0.5773502691896257, 'biolog': 0.5773502691896257, 'parent': 0.5773502691896257}\n",
      "{'adopt': 0.5773502691896257, 'biolog': 0.5773502691896257, 'parent': 0.5773502691896257}\n",
      "{'adopt': 0.5773502691896257, 'biolog': 0.5773502691896257, 'parent': 0.5773502691896257}\n",
      "{'black': 0.5773502691896257, 'bear': 0.5773502691896257, 'attack': 0.5773502691896257}\n",
      "{'black': 0.5773502691896257, 'bear': 0.5773502691896257, 'attack': 0.5773502691896257}\n",
      "{'black': 0.5773502691896257, 'bear': 0.5773502691896257, 'attack': 0.5773502691896257}\n",
      "{'viral': 0.7071067811865475, 'hepat': 0.7071067811865475}\n",
      "{'viral': 0.7071067811865475, 'hepat': 0.7071067811865475}\n",
      "{'risk': 0.5999882716312075, 'of': 0.5291768587249377, 'aspirin': 0.5999882716312075}\n",
      "{'risk': 0.5999882716312075, 'of': 0.5291768587249377, 'aspirin': 0.5999882716312075}\n",
      "{'risk': 0.5999882716312075, 'of': 0.5291768587249377, 'aspirin': 0.5999882716312075}\n",
      "{'alzheim': 0.5, 's': 0.5, 'drug': 0.5, 'treatment': 0.5}\n",
      "{'alzheim': 0.5, 's': 0.5, 'drug': 0.5, 'treatment': 0.5}\n",
      "{'alzheim': 0.5, 's': 0.5, 'drug': 0.5, 'treatment': 0.5}\n",
      "{'alzheim': 0.5, 's': 0.5, 'drug': 0.5, 'treatment': 0.5}\n",
      "{'land': 0.5773502691896257, 'mine': 0.5773502691896257, 'ban': 0.5773502691896257}\n",
      "{'land': 0.5773502691896257, 'mine': 0.5773502691896257, 'ban': 0.5773502691896257}\n",
      "{'land': 0.5773502691896257, 'mine': 0.5773502691896257, 'ban': 0.5773502691896257}\n",
      "{'airport': 0.7071067811865475, 'secur': 0.7071067811865475}\n",
      "{'airport': 0.7071067811865475, 'secur': 0.7071067811865475}\n",
      "{'diplomat': 0.7071067811865475, 'expuls': 0.7071067811865475}\n",
      "{'diplomat': 0.7071067811865475, 'expuls': 0.7071067811865475}\n",
      "{'polic': 0.7499770934074979, 'death': 0.6614638004940566}\n",
      "{'polic': 0.7499770934074979, 'death': 0.6614638004940566}\n",
      "{'abus': 0.5144883605221394, 'of': 0.45376776071215463, 'e': 0.5144883605221394, 'mail': 0.5144883605221394}\n",
      "{'abus': 0.5144883605221394, 'of': 0.45376776071215463, 'e': 0.5144883605221394, 'mail': 0.5144883605221394}\n",
      "{'abus': 0.5144883605221394, 'of': 0.45376776071215463, 'e': 0.5144883605221394, 'mail': 0.5144883605221394}\n",
      "{'abus': 0.5144883605221394, 'of': 0.45376776071215463, 'e': 0.5144883605221394, 'mail': 0.5144883605221394}\n",
      "{'oversea': 0.5773502691896257, 'tobacco': 0.5773502691896257, 'sale': 0.5773502691896257}\n",
      "{'oversea': 0.5773502691896257, 'tobacco': 0.5773502691896257, 'sale': 0.5773502691896257}\n",
      "{'oversea': 0.5773502691896257, 'tobacco': 0.5773502691896257, 'sale': 0.5773502691896257}\n",
      "{'educ': 0.7071067811865475, 'standard': 0.7071067811865475}\n",
      "{'educ': 0.7071067811865475, 'standard': 0.7071067811865475}\n",
      "{'wildlif': 0.7071067811865475, 'extinct': 0.7071067811865475}\n",
      "{'wildlif': 0.7071067811865475, 'extinct': 0.7071067811865475}\n",
      "{'agoraphobia': 1.0}\n",
      "{'metabol': 1.0}\n",
      "{'health': 0.5312340740155382, 'and': 0.39162619398421356, 'comput': 0.5312340740155382, 'termin': 0.5312340740155382}\n",
      "{'health': 0.5312340740155382, 'and': 0.39162619398421356, 'comput': 0.5312340740155382, 'termin': 0.5312340740155382}\n",
      "{'health': 0.5312340740155382, 'and': 0.39162619398421356, 'comput': 0.5312340740155382, 'termin': 0.5312340740155382}\n",
      "{'health': 0.5312340740155382, 'and': 0.39162619398421356, 'comput': 0.5312340740155382, 'termin': 0.5312340740155382}\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 讀檔\n",
    "Dot_txt = '.txt'\n",
    "query_path = 'C:/Users/dnc22/IR_HW1/queries/'\n",
    "doc_path = 'C:/Users/dnc22/IR_HW1/docs/'\n",
    "\n",
    "query_list_file = open('query_list' + Dot_txt,'r')\n",
    "doc_list_file = open('doc_list' + Dot_txt,'r')\n",
    "\n",
    "query_idf = dict()\n",
    "Query_tfidf = dict()\n",
    "\n",
    "for QueryFile_Name in query_list_file.readlines():\n",
    "    #讀入所有 Query Words \n",
    "    QueryFile_Name = QueryFile_Name.strip('\\n')\n",
    "    \n",
    "    query_file = open(query_path + QueryFile_Name + Dot_txt ,'r')\n",
    "    query = query_file.readlines()\n",
    "    query_file.close()\n",
    "    \n",
    "    query_dict = dict()\n",
    "    #截掉中間空白\n",
    "    query = [[word.lower() for word in doc.split()] for doc in query]\n",
    "    \n",
    "    query_tf = dict()\n",
    "    \n",
    "    idf = {}\n",
    "    for words in query[0]:\n",
    "        if (words in query_dict):\n",
    "            query_dict[words] += 1\n",
    "        else:\n",
    "            query_dict[words] = 1\n",
    "            \n",
    "        #用來計算 IDF，這篇文章出現過這個字就放到list\n",
    "        if (words not in idf):\n",
    "            idf[words] = 0\n",
    "            if(words not in query_idf):\n",
    "                query_idf[words] = 1\n",
    "            else:\n",
    "                query_idf[words] += 1\n",
    "    QueryCount = len(query_dict)\n",
    "    for words in query[0]:\n",
    "        # 算 query 的 TF\n",
    "        #query_tf[words] = query_dict[words] / QueryCount \n",
    "        query_tf[words] = query_dict[words] / 1 \n",
    "\n",
    "    Query_tfidf[QueryFile_Name] = query_tf\n",
    "    \n",
    "query_list_file.close()\n",
    "\n",
    "QueryFile_Count = len(Query_tfidf)\n",
    "\n",
    "# 計算 word 的 IDF  \n",
    "for sentense in query_idf:\n",
    "    query_idf[sentense] = math.log10(1+((QueryFile_Count+1) / (query_idf[sentense]+1)))\n",
    "    \n",
    "# 計算 word 的 TF-IDF\n",
    "for queryFile in Query_tfidf:\n",
    "    for sentense in Query_tfidf[queryFile]:\n",
    "        Query_tfidf[queryFile][sentense] = Query_tfidf[queryFile][sentense] * query_idf[sentense]\n",
    "        \n",
    "    #做正規化    \n",
    "    value_list = []\n",
    "    for key, values in Query_tfidf[queryFile].items():\n",
    "        value_list.append(values)\n",
    "    result_score = np.linalg.norm(value_list)\n",
    "    value_list /= result_score\n",
    "\n",
    "    #把 list的數字轉回 dict\n",
    "    index = 0\n",
    "    for sentense in Query_tfidf[queryFile]:\n",
    "        Query_tfidf[queryFile][sentense] = value_list[index]\n",
    "        index += 1\n",
    "        \n",
    "#print(Query_tfidf)        \n",
    "#-------------------\n",
    "\n",
    "Document_idf = dict()\n",
    "Document_tfidf = dict()\n",
    "\n",
    "#讀所有的 Document 檔名，再逐一打開每篇 Document\n",
    "for Document_Names in doc_list_file.readlines():\n",
    "    Document_Names = Document_Names.strip('\\n')\n",
    "    \n",
    "    document_tf = dict()\n",
    "    #打開 當篇 Docunment\n",
    "    doc_file = open(doc_path + Document_Names + Dot_txt, 'r')\n",
    "    document = doc_file.readlines()\n",
    "    doc_file.close()\n",
    "\n",
    "    document = [[word.lower() for word in doc.split()] \n",
    "               for doc in document]\n",
    "    \n",
    "    document_dict = dict()\n",
    "    idf = []\n",
    "\n",
    "    for words in document[0]:\n",
    "        if (words in document_dict):\n",
    "            document_dict[words] += 1\n",
    "        else:\n",
    "            document_dict[words] = 1 \n",
    "            \n",
    "        if (words not in idf):\n",
    "            if(words not in Document_idf):\n",
    "                Document_idf[words] = 1\n",
    "                idf.append(words)\n",
    "            else:\n",
    "                Document_idf[words] += 1\n",
    "                idf.append(words)\n",
    "    DocumentLength = len(document[0])\n",
    "    # 算 Document內的 Word 的 TF\n",
    "    for words in document[0]:\n",
    "        #document_tf[words] = document_dict[words] / DocumentLength\n",
    "        document_tf[words] = document_dict[words] / 1\n",
    "    Document_tfidf[Document_Names] = document_tf\n",
    "    \n",
    "DocuFile_Count = len(Document_tfidf)\n",
    "doc_list_file.close()\n",
    "\n",
    "#計算 Document 的 IDF\n",
    "for sentense in Document_idf:\n",
    "    Document_idf[sentense] = math.log(DocuFile_Count / Document_idf[sentense])\n",
    "    \n",
    "# 計算 Document 的 TF-IDF\n",
    "for docFile in Document_tfidf:\n",
    "    for sentense in Document_tfidf[docFile]:\n",
    "        Document_tfidf[docFile][sentense] = math.log10(Document_tfidf[docFile][sentense]) * Document_idf[sentense]\n",
    "        \n",
    "    #做正規化    \n",
    "    value_list = []\n",
    "    for key, values in Document_tfidf[docFile].items():\n",
    "        value_list.append(values)\n",
    "    result_score = np.linalg.norm(value_list)\n",
    "    value_list /= result_score\n",
    "\n",
    "    #把 list的數字轉回 dict\n",
    "    index = 0\n",
    "    for sentense in Document_tfidf[docFile]:\n",
    "        Document_tfidf[docFile][sentense] = value_list[index]\n",
    "        index += 1\n",
    "        \n",
    "#print(Document_tfidf)        \n",
    "#-------------------\n",
    "\n",
    "#算 Cos sim並寫檔\n",
    "qanswer = dict()\n",
    "#print(Document_idf)\n",
    "\n",
    "for queryfile in Query_tfidf:\n",
    "    qword_list = []\n",
    "    qvalue_list = []\n",
    "\n",
    "    #print(Query_tfidf)\n",
    "    for key, values in Query_tfidf[queryfile].items():\n",
    "        qword_list.append(key)\n",
    "        \n",
    "#         if(key in Document_idf):\n",
    "#             Query_tfidf[queryfile][key] = Query_tfidf[queryfile][key] * Document_idf[key]\n",
    "#         else:\n",
    "#             Query_tfidf[queryfile][key] = 0\n",
    "        qvalue_list.append(values)\n",
    "        \n",
    "        print(Query_tfidf[queryfile])\n",
    "\n",
    "    # queryfile = '301'\n",
    "    # qword_list = ['intern', 'organ', 'crime']\n",
    "    # qvalue_list = [0.333333, 0.3333333, 0.333333]\n",
    "    # print(qvalue_list)\n",
    "    \n",
    "    \n",
    "    # 湊出文件的向量\n",
    "    # docfile = FBIS3-10082\n",
    "    # dvalue_list = [0, 0, 0.039688973194635234]\n",
    "    danswerlist = []\n",
    "    for docfile in Document_tfidf:\n",
    "        dvalue_list = []\n",
    "        danswer = dict()\n",
    "        for word in qword_list:\n",
    "            if(word in Document_tfidf[docfile]):\n",
    "                dvalue_list.append(Document_tfidf[docfile][word])\n",
    "            else:\n",
    "                dvalue_list.append(0)\n",
    "        \n",
    "        # 兩向量做 Cos-sim\n",
    "        #print(qvalue_list, dvalue_list)\n",
    "        dot = sum(a * b for a, b in zip(qvalue_list, dvalue_list))\n",
    "        norm_a = sum(a * a for a in qvalue_list) ** 0.5\n",
    "        norm_b = sum(b * b for b in dvalue_list) ** 0.5\n",
    "        \n",
    "        # Cosine similarity\n",
    "        if(dot == 0 or (norm_a * norm_b) == 0):\n",
    "            cos_sim = 0\n",
    "        else:\n",
    "            cos_sim = dot / (norm_a * norm_b)\n",
    "        danswer[docfile] = cos_sim\n",
    "        danswerlist.append(danswer)\n",
    "    #print(danswerlist)\n",
    "    qanswer[queryfile] = danswerlist\n",
    "#print(qanswer)\n",
    "\n",
    "# 輸出檔案\n",
    "outPath = 'C:/Users/dnc22/IR_HW1/output.txt'\n",
    "with open(outPath, 'w') as f:\n",
    "    f.writelines('Query,RetrievedDocuments\\n')\n",
    "    \n",
    "    for queryfile in Query_tfidf:\n",
    "        answerdict = dict()\n",
    "        N = 0\n",
    "        while N < len(qanswer[queryfile]):\n",
    "            answerdict[str(list(qanswer[queryfile][N].keys())[0])] = list(qanswer[queryfile][N].values())[0]\n",
    "            N = N + 1\n",
    "        answerdict = sorted(answerdict.items(), key=lambda x:x[1], reverse = True)\n",
    "        #print(answerdict)\n",
    "        f.writelines(queryfile + \",\")\n",
    "        for DocuFile in answerdict:\n",
    "            f.writelines(DocuFile[0] + \" \")\n",
    "        f.writelines(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce3035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
